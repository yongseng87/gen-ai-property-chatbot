{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad593d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'property_database.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#Load main dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproperty_database.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#Prepare a dataset of unique Project Name and Street Name combinations, to reduce number of API calls\u001b[39;00m\n\u001b[32m     14\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33maddress\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mblock / building\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + df[\u001b[33m'\u001b[39m\u001b[33mstreet_name\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/GenAI/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/GenAI/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/GenAI/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/GenAI/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/GenAI/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'property_database.csv'"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import time \n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "#Load main dataset\n",
    "df = pd.read_csv(\"property_database.csv\")\n",
    "\n",
    "#Prepare a dataset of unique Project Name and Street Name combinations, to reduce number of API calls\n",
    "df['address'] = df['block / building'] + \" \" + df['street_name']\n",
    "unique_addresses = df['address'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "auth_token = \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjo5NDA0LCJmb3JldmVyIjpmYWxzZSwiaXNzIjoiT25lTWFwIiwiaWF0IjoxNzYwMTQ4OTc1LCJuYmYiOjE3NjAxNDg5NzUsImV4cCI6MTc2MDQwODE3NSwianRpIjoiMTk1OTM3NTktMDAwZi00Y2UwLTg2MzEtYjRhMWY0YmNlZTAyIn0.o54oKO8HUmBij7xFWf37Ly27vP_Ai-lpoT0Z7CnXAr8P42tphfTor5kQZTZupyOCrS3tlrYANqfM7V0euI_kLbkBDMdlMOVSPzj6MMMmZYQEOlnR2CP3APBQCGMIPuawNoL7d4qHMNZ4vYW6jXJV9LYfpx55c257iGLeS85QGvCyumiQoFZIdGI63WzPMb3SfYYIN4qcnEZ2Nsog-hDEo7ueBaQYl_P0oMxvBzatk5h19UrDpbU3b2b-s8JJnoxUkUhwD-ya8wGatvnS0LaGgqTUm2L9cs7eEdIbTFoh3fCLl-9X5L731A33eXpYDw5LQXi3OOC0OMk9oFnjnbpiCQ\"\n",
    "\n",
    "# Load existing cache if available \n",
    "\n",
    "try: \n",
    "    cache_df = pd.read_csv(\"geocoded_addresses.csv\") \n",
    "except FileNotFoundError: \n",
    "    cache_df = pd.DataFrame(columns=[\"address\", \"latitude\", \"longitude\"]) \n",
    "\n",
    "# Build a set for fast lookup \n",
    "cached_set = set(cache_df['address']) \n",
    "\n",
    "# Define geocoding function \n",
    "def get_lat_lng(address): \n",
    "    url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={address}&returnGeom=Y&getAddrDetails=Y&pageNum=1\" \n",
    "    headers = {\"Authorization\": auth_token} \n",
    "    try: \n",
    "        response = requests.get(url, headers=headers, timeout=5) \n",
    "        data = response.json() \n",
    "        if \"results\" in data and len(data[\"results\"]) > 0: \n",
    "            result = data[\"results\"][0] \n",
    "            return result.get(\"LATITUDE\"), result.get(\"LONGITUDE\") \n",
    "        else: return None, None \n",
    "\n",
    "    except Exception as e: print(f\"Error fetching {address}: {e}\") \n",
    "    return None, None \n",
    "\n",
    "# Geocode only missing addresses \n",
    " \n",
    "new_entries = [] \n",
    "\n",
    "for addr in tqdm(unique_addresses): \n",
    "    if addr in cached_set: \n",
    "        # Already cached â†’ skip \n",
    "        continue \n",
    "    \n",
    "    lat, lng = get_lat_lng(addr) \n",
    "    new_entries.append({\"address\": addr, \"latitude\": lat, \"longitude\": lng}) \n",
    "    \n",
    "    # Optional: sleep to respect API rate limits \n",
    "    time.sleep(0.1) \n",
    "    \n",
    "# Update cache \n",
    "if new_entries:\n",
    "    new_df = pd.DataFrame(new_entries)\n",
    "    cache_df = pd.concat([cache_df, new_df], ignore_index=True)\n",
    "    cache_df.to_csv(\"geocoded_addresses.csv\", index=False)\n",
    "\n",
    "# Merge cache back with all addresses \n",
    "all_addresses = pd.DataFrame({\"address\": unique_addresses}) \n",
    "all_addresses = all_addresses.merge(cache_df, on=\"address\", how=\"left\") \n",
    "\n",
    "# Second pass: retry failed with Street Name only\n",
    "\n",
    "failed_idx = all_addresses[all_addresses['latitude'].isna() | all_addresses['longitude'].isna()].index\n",
    "\n",
    "for idx in tqdm(failed_idx):\n",
    "    street_only = df.loc[df['address'] == all_addresses.at[idx, 'address'], 'street_name'].values[0]\n",
    "\n",
    "    # Check if cache has valid coordinates\n",
    "    cached_row = cache_df.loc[cache_df['address'] == street_only]\n",
    "    if not cached_row.empty and pd.notna(cached_row['latitude'].values[0]) and pd.notna(cached_row['longitude'].values[0]):\n",
    "        lat, lng = cached_row['latitude'].values[0], cached_row['longitude'].values[0]\n",
    "    else:\n",
    "        # Call API since cache is missing or invalid\n",
    "        lat, lng = get_lat_lng(street_only)\n",
    "\n",
    "        # Ensure scalars\n",
    "        if isinstance(lat, (tuple, list)):\n",
    "            lat = lat[0] if lat else None\n",
    "        if isinstance(lng, (tuple, list)):\n",
    "            lng = lng[0] if lng else None\n",
    "\n",
    "        # Append to cache safely\n",
    "        cache_df = pd.concat([cache_df, pd.DataFrame([{\n",
    "            \"address\": street_only,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lng\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        cache_df.to_csv(\"geocoded_addresses.csv\", index=False)\n",
    "\n",
    "    all_addresses.at[idx, 'latitude'] = lat\n",
    "    all_addresses.at[idx, 'longitude'] = lng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Measure travel distance to CBD using OSRM API\n",
    "\n",
    "# Path to travel distance cache\n",
    "CACHE_FILE = \"travel_distance_cache.csv\"\n",
    "\n",
    "# Load existing cache if available\n",
    "try:\n",
    "    dist_cache = pd.read_csv(CACHE_FILE)\n",
    "except FileNotFoundError:\n",
    "    dist_cache = pd.DataFrame(columns=[\"address\", \"dist_to_CBD_km_osrm\"])\n",
    "\n",
    "cached_set = set(dist_cache['address'])\n",
    "\n",
    "CBD_LAT = 1.283871989921002\n",
    "CBD_LON = 103.85149113157198\n",
    "\n",
    "CBD = (CBD_LON, CBD_LAT)  # (lon, lat)\n",
    "\n",
    "def get_osrm_distance(lon, lat, retries=3):\n",
    "    url = f\"http://router.project-osrm.org/route/v1/driving/{CBD[0]},{CBD[1]};{lon},{lat}?overview=false\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5).json()\n",
    "            if 'routes' in r and len(r['routes']) > 0:\n",
    "                distance_m = r['routes'][0]['distance']\n",
    "                return distance_m / 1000 # km\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {lon},{lat}: {e}, retry {attempt+1}\")\n",
    "            time.sleep(1)\n",
    "    return None, None\n",
    "\n",
    "new_entries = []\n",
    "\n",
    "unique_locations = all_addresses[['address', 'latitude', 'longitude']].drop_duplicates()\n",
    "\n",
    "for _, row in tqdm(unique_locations.iterrows(), total=len(unique_locations)):\n",
    "    addr = row['address']\n",
    "    lon, lat = row['longitude'], row['latitude']\n",
    "    \n",
    "    if addr in cached_set or pd.isna(lon) or pd.isna(lat):\n",
    "        continue\n",
    "    \n",
    "    dist_km = get_osrm_distance(lon, lat)\n",
    "    new_entries.append({\"address\": addr, \"dist_to_CBD_km_osrm\": dist_km})\n",
    "    cached_set.add(addr)\n",
    "    \n",
    "    time.sleep(0.1)  # optional: avoid throttling\n",
    "\n",
    "if new_entries:\n",
    "    new_df = pd.DataFrame(new_entries)\n",
    "    dist_cache = pd.concat([dist_cache, new_df], ignore_index=True)\n",
    "    dist_cache.to_csv(CACHE_FILE, index=False)\n",
    "\n",
    "# --- Merge cache back to main DataFrame ---\n",
    "all_addresses = all_addresses.merge(\n",
    "    dist_cache[['address', 'dist_to_CBD_km_osrm']],\n",
    "    on='address',\n",
    "    how='left',\n",
    "    suffixes=('', '_drop')\n",
    ")\n",
    "\n",
    "# Drop duplicate columns if any (from previous merges)\n",
    "all_addresses = all_addresses.loc[:, ~all_addresses.columns.str.endswith('_drop')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224209ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Convert school postal codes to latitudes and longitudes, using OneMap API\n",
    "\n",
    "auth_token = os.getenv(\"OneMapAuthToken\")\n",
    "\n",
    "# --- Load school CSV ---\n",
    "schools_df = pd.read_csv(\"Generalinformationofschools.csv\")\n",
    "schools_df['postal_code'] = schools_df['postal_code'].astype(str).str.zfill(6)\n",
    "\n",
    "# --- Load existing cache if available ---\n",
    "try:\n",
    "    cache_df = pd.read_csv(\"school_geocoded_postal_cache.csv\")\n",
    "except FileNotFoundError:\n",
    "    cache_df = pd.DataFrame(columns=[\"postal_code\", \"latitude\", \"longitude\"])\n",
    "\n",
    "# Ensure postal codes are strings with leading zeros if necessary\n",
    "# --- Build a set for fast lookup ---\n",
    "cache_df['postal_code'] = cache_df['postal_code'].astype(str).str.zfill(6)\n",
    "cached_set = set(cache_df['postal_code'])\n",
    "\n",
    "# --- Geocoding function ---\n",
    "def get_lat_lng(postal_code):\n",
    "    url = f\"https://www.onemap.gov.sg/api/common/elastic/search?searchVal={postal_code}&returnGeom=Y&getAddrDetails=Y&pageNum=1\"\n",
    "    headers = {\"Authorization\": auth_token}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        data = response.json()\n",
    "        if \"results\" in data and len(data[\"results\"]) > 0:\n",
    "            result = data[\"results\"][0]\n",
    "            return result.get(\"LATITUDE\"), result.get(\"LONGITUDE\")\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {postal_code}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Geocode only missing postal codes ---\n",
    "new_entries = []\n",
    "\n",
    "for pc in tqdm(schools_df['postal_code']):\n",
    "    if pc in cached_set:\n",
    "        continue\n",
    "    \n",
    "    lat, lng = get_lat_lng(pc)\n",
    "    new_entries.append({\"postal_code\": pc, \"latitude\": lat, \"longitude\": lng})\n",
    "    cached_set.add(pc)\n",
    "    \n",
    "    time.sleep(0.1)  # optional: avoid rate limits\n",
    "\n",
    "# --- Update cache ---\n",
    "if new_entries:\n",
    "    new_df = pd.DataFrame(new_entries)\n",
    "    cache_df = pd.concat([cache_df, new_df], ignore_index=True)\n",
    "    cache_df.to_csv(\"school_geocoded_postal_cache.csv\", index=False)\n",
    "\n",
    "# --- Merge cache back to schools DataFrame ---\n",
    "schools_df = schools_df.merge(\n",
    "    cache_df,\n",
    "    left_on='postal_code',\n",
    "    right_on='postal_code',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Second pass: retry failed with School Name only ---\n",
    "# Identify rows where latitude or longitude is missing\n",
    "failed_idx = schools_df[schools_df['latitude'].isna() | schools_df['longitude'].isna()].index\n",
    "\n",
    "for idx in tqdm(failed_idx):\n",
    "    school_name = schools_df.at[idx, 'school_name']\n",
    "\n",
    "    # Check if cache has valid coordinates\n",
    "    cached_row = cache_df.loc[cache_df['school_name'] == school_name]\n",
    "    if not cached_row.empty and pd.notna(cached_row['latitude'].values[0]) and pd.notna(cached_row['longitude'].values[0]):\n",
    "        lat, lng = cached_row['latitude'].values[0], cached_row['longitude'].values[0]\n",
    "    else:\n",
    "        # Call API since cache is missing or invalid\n",
    "        lat, lng = get_lat_lng(school_name)\n",
    "\n",
    "        # Ensure scalars (sometimes API returns tuple/list)\n",
    "        if isinstance(lat, (tuple, list)):\n",
    "            lat = lat[0] if lat else None\n",
    "        if isinstance(lng, (tuple, list)):\n",
    "            lng = lng[0] if lng else None\n",
    "\n",
    "        # Append to cache safely\n",
    "        cache_df = pd.concat([cache_df, pd.DataFrame([{\n",
    "            \"school_name\": school_name,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lng\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        # Save updated cache\n",
    "        cache_df.to_csv(\"school_geocoded_postal_cache.csv\", index=False)\n",
    "\n",
    "    # Update the main DataFrame\n",
    "\n",
    "    schools_df.at[idx, 'latitude'] = lat\n",
    "    schools_df.at[idx, 'longitude'] = lng\n",
    "\n",
    "schools_df['school_name'] = schools_df['school_name_y'].combine_first(schools_df['school_name_x'])\n",
    "schools_df = schools_df.drop(columns=['school_name_x', 'school_name_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert latitude and longitude to numeric, in case there are any non-numeric values\n",
    "schools_df['latitude'] = pd.to_numeric(schools_df['latitude'], errors='coerce')\n",
    "schools_df['longitude'] = pd.to_numeric(schools_df['longitude'], errors='coerce')\n",
    "\n",
    "# Ensure numeric types\n",
    "all_addresses['latitude'] = pd.to_numeric(all_addresses['latitude'], errors='coerce')\n",
    "all_addresses['longitude'] = pd.to_numeric(all_addresses['longitude'], errors='coerce')\n",
    "\n",
    "# Vectorized Haversine function\n",
    "def haversine_vec(lat1, lon1, lat2_arr, lon2_arr):\n",
    "    lat1, lon1 = np.radians(lat1), np.radians(lon1)\n",
    "    lat2, lon2 = np.radians(lat2_arr), np.radians(lon2_arr)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371 * c  # distance in km\n",
    "\n",
    "# Prepare arrays\n",
    "prop_lat = all_addresses['latitude'].values\n",
    "prop_lon = all_addresses['longitude'].values\n",
    "school_lat = schools_df['latitude'].values\n",
    "school_lon = schools_df['longitude'].values\n",
    "\n",
    "# Initialize lists\n",
    "nearest_school_dist = []\n",
    "nearest_school_name = []\n",
    "\n",
    "# Loop through each property\n",
    "for lat_p, lon_p in zip(prop_lat, prop_lon):\n",
    "    distances = haversine_vec(lat_p, lon_p, school_lat, school_lon)\n",
    "    min_idx = distances.argmin()  # nearest school index\n",
    "    \n",
    "    nearest_school_dist.append(distances[min_idx])\n",
    "    nearest_school_name.append(schools_df.loc[min_idx, 'school_name'])\n",
    "\n",
    "# Add columns to all_addresses\n",
    "all_addresses['nearest_school_dist_km'] = nearest_school_dist\n",
    "all_addresses['nearest_school_name'] = nearest_school_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load GeoJSON\n",
    "with open(\"LTAMRTStationExitGEOJSON.geojson\", \"r\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "features = geojson_data['features']\n",
    "\n",
    "mrt_list = []\n",
    "pattern = r\"<th>STATION_NA<\\/th>\\s*<td>(.*?)<\\/td>\"\n",
    "\n",
    "for feat in features:\n",
    "    coords = feat['geometry']['coordinates']  # [lon, lat, ...]\n",
    "    desc = feat['properties']['Description']\n",
    "    \n",
    "    match = re.search(pattern, desc)\n",
    "    station_name = match.group(1) if match else None\n",
    "    \n",
    "    mrt_list.append({\n",
    "        'mrt_lon': coords[0],\n",
    "        'mrt_lat': coords[1],\n",
    "        'StationName': station_name\n",
    "    })\n",
    "\n",
    "mrt_df = pd.DataFrame(mrt_list)\n",
    "\n",
    "mrt_df['mrt_lat'] = pd.to_numeric(mrt_df['mrt_lat'], errors='coerce')\n",
    "mrt_df['mrt_lon'] = pd.to_numeric(mrt_df['mrt_lon'], errors='coerce')\n",
    "\n",
    "# Calculate nearest MRT station for each property, as well as distance\n",
    "nearest_mrt_dist = []\n",
    "nearest_mrt_name = []\n",
    "\n",
    "mrt_lat = mrt_df['mrt_lat'].values\n",
    "mrt_lon = mrt_df['mrt_lon'].values\n",
    "\n",
    "for lat_p, lon_p in zip(prop_lat, prop_lon):\n",
    "    distances = haversine_vec(lat_p, lon_p, mrt_lat, mrt_lon)\n",
    "    min_idx = distances.argmin()\n",
    "    nearest_mrt_dist.append(distances[min_idx])\n",
    "    nearest_mrt_name.append(mrt_df.loc[min_idx, 'StationName'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652deee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_addresses['nearest_mrt_dist_km'] = nearest_mrt_dist\n",
    "all_addresses['nearest_mrt_name'] = nearest_mrt_name\n",
    "\n",
    "df = df.merge(all_addresses, on='address', how='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
