{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f76997d",
   "metadata": {},
   "source": [
    "### All Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5072ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Memory & Chains\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain, LLMChain, RetrievalQA\n",
    "\n",
    "# Retrieval & Vector Stores\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Tools & Agents\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25402c",
   "metadata": {},
   "source": [
    "### Environment Setup and API Key Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19db253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key found and loaded\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEY not found in environment variables. Please set it in your .env file.\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key found and loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8afec0",
   "metadata": {},
   "source": [
    "## 1. Models - Basic Setup\n",
    "**Models are the AI brains (like GPT-4) that understand your questions and generate intelligent responses.**\n",
    "\n",
    "### Understanding Message Types in LangChain\n",
    "\n",
    "This is how LangChain keeps track of who said what in a conversation â€” it uses different message types for different roles:\n",
    "\n",
    "- **HumanMessage** â†’ input from the user (like you typing a query).\n",
    "- **AIMessage** â†’ responses generated by the AI model.\n",
    "- **SystemMessage** â†’ system-level instructions (e.g., context, rules, or prompts that guide the AI's behavior).\n",
    "\n",
    "So in your code:\n",
    "```python\n",
    "response = llm.invoke([HumanMessage(content=test_input)])\n",
    "```\n",
    "\n",
    "You're passing the model a list of messages, where the first message is from the human (you). The model then replies with an AIMessage.\n",
    "\n",
    "ðŸ‘‰ If you just passed a raw string, it wouldn't know who the message was from, but with HumanMessage, it knows it's your input in a dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d98b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”µ INPUT TO GPT:\n",
      "Query: What are the top 3 benefits of online learning platforms?\n",
      "\n",
      "ðŸ”µ CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT:\n",
      "Online learning platforms offer numerous advantages, but here are the top three benefits:\n",
      "\n",
      "1. **Flexibility and Convenience**: Online learning allows students to access courses and materials at their own pace and on their own schedule. This flexibility is particularly beneficial for those balancing work, family, or other commitments, as it enables learners to study when it suits them best.\n",
      "\n",
      "2. **Wide Range of Courses and Resources**: Online platforms provide access to a vast array of courses across various subjects and disciplines, often from renowned institutions and experts. This diversity allows learners to explore new interests, acquire new skills, and pursue professional development opportunities that may not be available locally.\n",
      "\n",
      "3. **Cost-Effectiveness**: Many online learning platforms offer affordable or even free courses, reducing the financial burden associated with traditional education. Additionally, learners can save on commuting and accommodation costs, making education more accessible to a broader audience.\n",
      "\n",
      "These benefits contribute to the growing popularity of online learning as a viable alternative to traditional educational methods.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_input = \"What are the top 3 benefits of online learning platforms?\"\n",
    "\n",
    "print(\"ðŸ”µ INPUT TO GPT:\")\n",
    "print(f\"Query: {test_input}\")\n",
    "\n",
    "print(\"\\nðŸ”µ CALLING GPT-4O-MINI...\")\n",
    "response = llm.invoke([HumanMessage(content=test_input)])\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cebc713",
   "metadata": {},
   "source": [
    "## 2. Prompts and Templates\n",
    "**Prompts are structured instructions with placeholders that tell the model exactly what to do with your data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8b8b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”µ INPUT TO GPT:\n",
      "Template: Property Support Analysis\n",
      "User Query: I don't understand the tenancy agreements and I need to sign tomorrow!\n",
      "Formatted Messages: ['You are a professional property agent. Be patient and helpful.', \"Analyze this user query: I don't understand the tenancy agreements and I need to sign tomorrow!\", 'Provide urgency level (1-5) and suggested response category.']\n",
      "\n",
      "ðŸ”µ CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT:\n",
      "Urgency Level: 5 (High urgency, as the user needs to sign the agreement tomorrow)\n",
      "\n",
      "Suggested Response Category: Clarification and Guidance on Tenancy Agreements\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Basic Property Support Template\n",
    "user_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a professional property agent. Be patient and helpful.\"),\n",
    "    (\"human\", \"Analyze this user query: {user_query}\"),\n",
    "    (\"human\", \"Provide urgency level (1-5) and suggested response category.\")\n",
    "])\n",
    "\n",
    "# Format and use the prompt\n",
    "user_query = \"I don't understand the tenancy agreements and I need to sign tomorrow!\"\n",
    "formatted_prompt = user_prompt.format_messages(user_query=user_query)\n",
    "\n",
    "print(\"ðŸ”µ INPUT TO GPT:\")\n",
    "print(\"Template: Property Support Analysis\")\n",
    "print(f\"User Query: {user_query}\")\n",
    "print(f\"Formatted Messages: {[msg.content for msg in formatted_prompt]}\")\n",
    "\n",
    "print(\"\\nðŸ”µ CALLING GPT-4O-MINI...\")\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50ef0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ INPUT TO GPT:\n",
      "Template: Multi-role Support Response\n",
      "Variables: {'role': 'Senior Property Agent', 'platform': 'Real Estage Consultant Platform', 'tone': 'patient and supportive', 'issue': 'Struggling with understanding tenancy agreements', 'context': 'User is new to property rental processes, but need to sign tenancy soon'}\n",
      "\n",
      "ðŸ”µ CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT:\n",
      "Absolutely, I understand that navigating tenancy agreements can be quite overwhelming, especially if you're new to the property rental process. It's completely normal to have questions and concerns.\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Multi-role Conversation Template\n",
    "support_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {role} at {platform}. Use {tone} tone.\"),\n",
    "    (\"human\", \"Student issue: {issue}\"),\n",
    "    (\"human\", \"Previous context: {context}\"),\n",
    "    (\"human\", \"Generate a response that addresses the issue and follows up appropriately.\")\n",
    "])\n",
    "\n",
    "# Example usage\n",
    "response_inputs = {\n",
    "    \"role\": \"Senior Property Agent\",\n",
    "    \"platform\": \"Real Estage Consultant Platform\", \n",
    "    \"tone\": \"patient and supportive\",\n",
    "    \"issue\": \"Struggling with understanding tenancy agreements\",\n",
    "    \"context\": \"User is new to property rental processes, but need to sign tenancy soon\"\n",
    "}\n",
    "\n",
    "formatted_chat = support_template.format_messages(**response_inputs)\n",
    "\n",
    "print(\"\\nðŸ”µ INPUT TO GPT:\")\n",
    "print(\"Template: Multi-role Support Response\")\n",
    "print(f\"Variables: {response_inputs}\")\n",
    "\n",
    "print(\"\\nðŸ”µ CALLING GPT-4O-MINI...\")\n",
    "response = llm.invoke(formatted_chat)\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT:\")\n",
    "print(response.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97c417",
   "metadata": {},
   "source": [
    "## 3. Chains - Connecting Components\n",
    "**Chains link multiple steps together (prompt â†’ model â†’ parser) to create reusable AI workflows.**\n",
    "\n",
    "### Understanding the Pipe Operator (|) in LangChain\n",
    "\n",
    "#### What does | mean here?\n",
    "\n",
    "Yes â€” in LangChain's new \"LangChain Expression Language\" (LCEL), the `|` is operator overloading for pipe composition.\n",
    "\n",
    "It works a lot like a Unix shell pipeline (`cmd1 | cmd2 | cmd3`), but instead of passing raw text, each component in LangChain has a well-defined input/output schema. The pipe (`|`) just wires them together.\n",
    "\n",
    "So this:\n",
    "```python\n",
    "solution_template | llm | StrOutputParser()\n",
    "```\n",
    "\n",
    "means:\n",
    "1. Take the PromptTemplate (solution_template)\n",
    "2. Format it with the given inputs â†’ produces a prompt string (or messages)\n",
    "3. Send it to llm (the LLM model) â†’ produces an LLM response\n",
    "4. Pass the response into StrOutputParser() â†’ returns a clean string\n",
    "\n",
    "Each `|` passes the output of the left component as input to the right component, creating a seamless data flow pipeline.\n",
    "\n",
    "#### Chain vs Manual Steps Comparison\n",
    "\n",
    "**âŒ Manual Approach (3 separate steps):**\n",
    "```python\n",
    "# Step 1: Format the prompt\n",
    "formatted_prompt = prompt.format_messages(metric=metric_input)\n",
    "\n",
    "# Step 2: Call the LLM\n",
    "raw_response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Step 3: Parse the output\n",
    "result = parser.invoke(raw_response)\n",
    "```\n",
    "\n",
    "**âœ… Chain Approach (1 simple step):**\n",
    "```python\n",
    "# All steps combined into one seamless pipeline\n",
    "analysis_chain = prompt | llm | parser\n",
    "result = analysis_chain.invoke({\"metric\": metric_input})\n",
    "```\n",
    "\n",
    "**Benefits of Chains:**\n",
    "- **Less Code**: 1 line instead of 3 separate operations\n",
    "- **No Intermediate Variables**: No need to manage `formatted_prompt` or `raw_response`\n",
    "- **Automatic Data Flow**: Each component's output automatically becomes the next component's input\n",
    "- **Reusable Pipeline**: Create once, use multiple times with different inputs\n",
    "- **Error Handling**: Built-in error propagation through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cc9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”µ INPUT TO GPT:\n",
      "Chain: Property Data Analysis\n",
      "Metric: property is 20 years old with recent renovation\n",
      "\n",
      "ðŸ”µ CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT:\n",
      "When analyzing a property that is 20 years old with recent renovations, several key factors should be considered:\n",
      "\n",
      "1. **Age of the Property**: \n",
      "   - A 20-year-old property is typically considered to be in the middle of its lifecycle. It may have some wear and tear, but if it has been well-maintained, it can still be a solid investment.\n",
      "   - The age can also affect the building codes and standards it was built under, which may differ from current regulations.\n",
      "\n",
      "2. **Recent Renovations**:\n",
      "   - Renovations can significantly enhance the property's value and appeal. It's important to assess the quality and extent of the renovations. Were they cosmetic (like new paint and flooring) or structural (like updated plumbing, electrical systems, or roof)?\n",
      "   - Renovations can also improve energy efficiency, which can lower utility costs and appeal to environmentally conscious buyers.\n",
      "\n",
      "3. **Market Appeal**:\n",
      "   - A renovated property may attract a wider range of buyers, including first-time homeowners and investors looking for rental properties. The appeal of modern amenities and aesthetics can lead to quicker sales and potentially higher offers.\n",
      "   - The location and neighborhood trends should also be considered. If the area is up-and-coming, the property may appreciate in value more rapidly.\n",
      "\n",
      "4. **Maintenance and Upkeep**:\n",
      "   - Even with renovations, it's essential to consider the ongoing maintenance needs of a 20-year-old property. Systems such as HVAC, plumbing, and roofing may still be approaching the end of their useful life, depending on the quality of the original installation and the renovations made.\n",
      "\n",
      "5. **Comparative Market Analysis (CMA)**:\n",
      "   - Conducting a CMA will help determine how the property compares to similar properties in the area, both in terms of age and renovations. This analysis can provide insights into pricing strategies and potential return on investment.\n",
      "\n",
      "6. **Financing and Insurance**:\n",
      "   - Lenders may have specific requirements for older properties, even if they have been renovated. It's important to check if the renovations meet current standards and if they will affect financing options.\n",
      "   - Insurance costs may also vary based on the age of the property and the extent of renovations.\n",
      "\n",
      "7. **Potential for Future Renovations**:\n",
      "   - Consider whether there is potential for further renovations or expansions that could increase the property's value. This could be particularly appealing to investors.\n",
      "\n",
      "In summary, a 20-year-old property with recent renovations can be a valuable asset, but it's crucial to evaluate the quality of the renovations, the property's overall condition, market trends, and potential future needs. This comprehensive analysis will help in making informed decisions regarding pricing, marketing, and investment potential.\n"
     ]
    }
   ],
   "source": [
    "# Create tenancy data analysis chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a professional property agent.\"),\n",
    "    (\"human\", \"Analyze this property metric: {metric}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Create chain using pipe operator\n",
    "analysis_chain = prompt | llm | parser\n",
    "\n",
    "# Use the chain\n",
    "metric_input = \"property is 20 years old with recent renovation\"\n",
    "\n",
    "print(\"ðŸ”µ INPUT TO GPT:\")\n",
    "print(\"Chain: Property Data Analysis\")\n",
    "print(f\"Metric: {metric_input}\")\n",
    "\n",
    "print(\"\\nðŸ”µ CALLING GPT-4O-MINI...\")\n",
    "result = analysis_chain.invoke({\"metric\": metric_input})\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286eda2a",
   "metadata": {},
   "source": [
    "## 4. Memory - Conversation Context\n",
    "**Memory stores conversation history so the AI remembers what was discussed earlier in the chat.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f79c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”µ INPUT TO GPT:\n",
      "Conversation with Memory - Turn 1\n",
      "Student: Hi, I'm Alice and I'm struggling with understanding my tenancy agreements\n",
      "\n",
      "ðŸ”µ CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT:\n",
      "AI: Hello Alice! It's great to meet you. Tenancy agreements can definitely be a bit tricky to navigate. They usually outline the rights and responsibilities of both the landlord and the tenant, including things like rent payment, maintenance responsibilities, and rules about the property. What specific aspects of your tenancy agreement are you struggling with? Are there particular clauses or terms that are confusing? I'm here to help!\n",
      "\n",
      "ðŸ”µ INPUT TO GPT:\n",
      "Conversation with Memory - Turn 2\n",
      "Student: What's my name and what am I busy with?\n",
      "\n",
      "ðŸ”µ CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT:\n",
      "AI: Your name is Alice, and you're currently busy with understanding your tenancy agreements. It sounds like you're trying to get a clearer grasp on the details and terms involved. If you have any specific questions or sections you'd like to discuss, feel free to share!\n",
      "\n",
      "ðŸ“ Memory Buffer:\n",
      "Human: Hi, I'm Alice and I'm struggling with understanding my tenancy agreements\n",
      "AI: Hello Alice! It's great to meet you. Tenancy agreements can definitely be a bit tricky to navigate. They usually outline the rights and responsibilities of both the landlord and the tenant, including things like rent payment, maintenance responsibilities, and rules about the property. What specific aspects of your tenancy agreement are you struggling with? Are there particular clauses or terms that are confusing? I'm here to help!\n",
      "Human: What's my name and what am I busy with?\n",
      "AI: Your name is Alice, and you're currently busy with understanding your tenancy agreements. It sounds like you're trying to get a clearer grasp on the details and terms involved. If you have any specific questions or sections you'd like to discuss, feel free to share!\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "input1 = \"Hi, I'm Alice and I'm struggling with understanding my tenancy agreements\"\n",
    "\n",
    "print(\"ðŸ”µ INPUT TO GPT:\")\n",
    "print(\"Conversation with Memory - Turn 1\")\n",
    "print(f\"Student: {input1}\")\n",
    "\n",
    "print(\"\\nðŸ”µ CALLING GPT-4O-MINI...\")\n",
    "response1 = conversation.invoke({\"input\": input1})[\"response\"]\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT:\")\n",
    "print(\"AI:\", response1)\n",
    "\n",
    "input2 = \"What's my name and what am I busy with?\"\n",
    "\n",
    "print(\"\\nðŸ”µ INPUT TO GPT:\")\n",
    "print(\"Conversation with Memory - Turn 2\")\n",
    "print(f\"Student: {input2}\")\n",
    "\n",
    "print(\"\\nðŸ”µ CALLING GPT-4O-MINI...\")\n",
    "response2 = conversation.invoke({\"input\": input2})[\"response\"]\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT:\")\n",
    "print(\"AI:\", response2)\n",
    "\n",
    "print(\"\\nðŸ“ Memory Buffer:\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29680f",
   "metadata": {},
   "source": [
    "## 5. Retrieval - Knowledge Base Integration\n",
    "**Retrieval searches through documents/databases to find relevant information before answering questions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4179b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Creating Knowledge Base...\n"
     ]
    }
   ],
   "source": [
    "# Sample educational policies for knowledge base\n",
    "educational_policies = [\n",
    "    \"Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds available until week 4.\",\n",
    "    \"Grade Appeal Process: Students must submit grade appeals within 2 weeks of grade posting. Appeals are reviewed by academic committee.\",\n",
    "    \"Academic Probation: Students with GPA below 2.0 for two consecutive semesters are placed on academic probation.\",\n",
    "    \"Extension Policy: Course extensions up to 30 days require instructor approval. Extensions over 30 days need academic advisor approval.\",\n",
    "    \"Technical Support: All students receive email support within 24 hours. Priority support available for premium students.\",\n",
    "    \"Graduation Requirements: Students must complete 120 credits with minimum 2.0 GPA and all core course requirements.\"\n",
    "]\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "print(\"ðŸ“š Creating Knowledge Base...\")\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=educational_policies,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1cfc6",
   "metadata": {},
   "source": [
    "### Understanding RetrievalQA.from_chain_type() Components\n",
    "\n",
    "```python\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "```\n",
    "\n",
    "**ðŸ¤” Why `from_chain_type()` instead of direct initialization?**\n",
    "\n",
    "`RetrievalQA` is a complex chain that needs to coordinate multiple steps:\n",
    "1. **Retrieve** relevant documents from vector store\n",
    "2. **Combine** retrieved text with user question  \n",
    "3. **Generate** answer using LLM\n",
    "\n",
    "`from_chain_type()` is a **factory method** that automatically builds the right chain configuration for you, instead of manually wiring these components together.\n",
    "\n",
    "**ðŸ—‚ï¸ What is `chain_type=\"stuff\"`?**\n",
    "\n",
    "The \"stuff\" method determines **how retrieved documents are combined** before sending to the LLM:\n",
    "\n",
    "| Chain Type | How It Works | Best For | Example |\n",
    "|------------|--------------|----------|---------|\n",
    "| **\"stuff\"** | Concatenates ALL retrieved docs into one prompt | Short docs, simple questions | \"Here are 3 policy documents: [doc1][doc2][doc3]. Question: What's the drop policy?\" |\n",
    "| **\"map_reduce\"** | Summarizes each doc separately, then combines summaries | Long docs, complex analysis | Step 1: Summarize each doc â†’ Step 2: Combine summaries â†’ Answer |\n",
    "| **\"refine\"** | Builds answer iteratively, refining with each doc | Sequential reasoning needed | Answer using doc1 â†’ Refine answer with doc2 â†’ Final refine with doc3 |\n",
    "| **\"map_rerank\"** | Scores each doc's relevance, uses highest-scoring one | When only 1 best source needed | Score doc1: 0.9, doc2: 0.7, doc3: 0.8 â†’ Use doc1 only |\n",
    "\n",
    "**\"stuff\"** is the **most common and fastest** approach for typical Q&A scenarios.\n",
    "\n",
    "**ðŸ” What is `vectorstore.as_retriever()`?**\n",
    "\n",
    "**Vector Store** vs **Retriever** serve different purposes:\n",
    "\n",
    "- **VectorStore** (`Chroma`) = Database that stores and searches embeddings\n",
    "  - Methods: `.similarity_search()`, `.add_texts()`, `.persist()`\n",
    "  - Direct database operations\n",
    "\n",
    "- **Retriever** = Standardized interface for getting relevant documents  \n",
    "  - Methods: `.get_relevant_documents()`, `.invoke()`\n",
    "  - Works with any retrieval system (vector, keyword, hybrid)\n",
    "\n",
    "**Why convert?**\n",
    "```python\n",
    "# âŒ RetrievalQA expects a \"retriever\" interface, not a \"vectorstore\" \n",
    "RetrievalQA.from_chain_type(retriever=vectorstore)  # TypeError!\n",
    "\n",
    "# âœ… Convert vectorstore to retriever interface\n",
    "RetrievalQA.from_chain_type(retriever=vectorstore.as_retriever())  # Works!\n",
    "```\n",
    "\n",
    "**Optional retriever configuration:**\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",     # or \"mmr\" (max marginal relevance) \n",
    "    search_kwargs={\"k\": 3}        # return top 3 documents\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12b8f68",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ INPUT TO RETRIEVAL QA:\n",
      "Question: What is the course drop policy?\n",
      "Action: Search knowledge base + Generate answer\n",
      "\n",
      "ðŸ”µ SEARCHING KNOWLEDGE BASE AND CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT (with retrieved context):\n",
      "Answer: Students can drop courses within 2 weeks for a full refund. Partial refunds are available until week 4.\n",
      "Sources: ['Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds availa...', 'Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds availa...', 'Course Drop Policy: Students can drop courses within 2 weeks for full refund. Partial refunds availa...', 'Extension Policy: Course extensions up to 30 days require instructor approval. Extensions over 30 da...']\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "question = \"What is the course drop policy?\"\n",
    "\n",
    "print(f\"\\nðŸ”µ INPUT TO RETRIEVAL QA:\")\n",
    "print(f\"Question: {question}\")\n",
    "print(\"Action: Search knowledge base + Generate answer\")\n",
    "\n",
    "print(\"\\nðŸ”µ SEARCHING KNOWLEDGE BASE AND CALLING GPT-4O-MINI...\")\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(\"\\nðŸ¤– OUTPUT FROM GPT (with retrieved context):\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(f\"Sources: {[doc.page_content[:100] + '...' for doc in result['source_documents']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb44163",
   "metadata": {},
   "source": [
    "## 6. PDF Processing with Chroma DB - Document Q&A\n",
    "**PDF Processing extracts text from PDFs and creates searchable vector databases for intelligent Q&A.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa76e634",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting PDF Processing Pipeline...\n",
      "ðŸ“š Loading PDF document...\n",
      "File: examples/Track_B_Tenancy_Agreement.pdf\n",
      "âœ… Loaded 10 pages from PDF\n",
      "ðŸ“ Split into 37 text chunks\n",
      "ðŸ” Creating vector embeddings...\n",
      "ðŸ’¾ Vector database saved to: ./pdf_knowledge_base\n",
      "âœ… PDF Q&A system created successfully\n",
      "\n",
      "============================================================\n",
      "TESTING PDF Q&A SYSTEM\n",
      "============================================================\n",
      "\n",
      "ðŸ”µ INPUT TO PDF Q&A SYSTEM:\n",
      "Question: What is the address of the premise?\n",
      "Action: Search PDF content + Generate answer\n",
      "\n",
      "ðŸ”µ SEARCHING PDF DATABASE AND CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT (with PDF context):\n",
      "Answer: I don't know.\n",
      "ðŸ“„ Sources: Page 7 of PDF\n",
      "ðŸ“ Source Text Preview: address specified herein at the last known place of abode or business. A notice sent by registered letter shall deemed\n",
      "to be given at the time when it...\n",
      "\n",
      "ðŸ”µ INPUT TO PDF Q&A SYSTEM:\n",
      "Question: Can I hang a picture on the wall?\n",
      "Action: Search PDF content + Generate answer\n",
      "\n",
      "ðŸ”µ SEARCHING PDF DATABASE AND CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT (with PDF context):\n",
      "Answer: Yes, you can hang a picture on the wall, but you must obtain the landlord's written consent first. Additionally, you will need to remove any nails, screws, or hooks and patch the holes with white putty before the end of the tenancy.\n",
      "ðŸ“„ Sources: Page 4 of PDF\n",
      "ðŸ“ Source Text Preview: At the expiration or earlier termination of the Tenancy Agreement to peaceably and quietly deliver up to the Landlord\n",
      "the Premises in like condition a...\n",
      "\n",
      "ðŸ”µ INPUT TO PDF Q&A SYSTEM:\n",
      "Question: Can I claim my deposit back if I end the tenancy early?\n",
      "Action: Search PDF content + Generate answer\n",
      "\n",
      "ðŸ”µ SEARCHING PDF DATABASE AND CALLING GPT-4O-MINI...\n",
      "\n",
      "ðŸ¤– OUTPUT FROM GPT (with PDF context):\n",
      "Answer: It depends on the circumstances under which you end the tenancy. If you lawfully terminate the Tenancy Agreement or exercise the diplomatic clause, you may be required to reimburse the landlord for the commission paid to the agency on a pro-rata basis for the remaining unfulfilled term of the tenancy. The landlord has the right to deduct this reimbursement from your security deposit. Therefore, whether you can claim your deposit back may depend on any deductions made for this reimbursement.\n",
      "ðŸ“„ Sources: Page 6 of PDF\n",
      "ðŸ“ Source Text Preview: Landlord has the right, but not the obligation, to deduct such reimbursement of the commission from the security\n",
      "deposit as stipulated by Clause 2 abo...\n"
     ]
    }
   ],
   "source": [
    "def load_and_process_pdf(pdf_path):\n",
    "    \"\"\"Load PDF and process it for Chroma DB\"\"\"\n",
    "    \n",
    "    print(\"ðŸ“š Loading PDF document...\")\n",
    "    print(f\"File: {pdf_path}\")\n",
    "    \n",
    "    # Load PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(pages)} pages from PDF\")\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    docs = text_splitter.split_documents(pages)\n",
    "    print(f\"ðŸ“ Split into {len(docs)} text chunks\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    embeddings_pdf = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    \n",
    "    # Clear any existing chroma database (commented out to prevent errors on rerun)\n",
    "    db_path = \"./pdf_knowledge_base\"\n",
    "    # if os.path.exists(db_path):\n",
    "    #     shutil.rmtree(db_path)\n",
    "    #     print(\"ðŸ§¹ Cleared existing database\")\n",
    "    \n",
    "    # Create Chroma vector store\n",
    "    print(\"ðŸ” Creating vector embeddings...\")\n",
    "    vectorstore_pdf = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings_pdf,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    # Persist the database\n",
    "    vectorstore_pdf.persist()\n",
    "    print(f\"ðŸ’¾ Vector database saved to: {db_path}\")\n",
    "    \n",
    "    return vectorstore_pdf\n",
    "\n",
    "def create_pdf_qa_system(vectorstore_pdf):\n",
    "    \"\"\"Create Q&A system for PDF documents\"\"\"\n",
    "    \n",
    "    # Create retrieval QA chain\n",
    "    qa_chain_pdf = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore_pdf.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}  # Return top 3 relevant chunks\n",
    "        ),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… PDF Q&A system created successfully\")\n",
    "    return qa_chain_pdf\n",
    "\n",
    "# Process the PDF (assuming educational_course_handbook.pdf exists)\n",
    "pdf_file_path = \"examples/Track_B_Tenancy_Agreement.pdf\"\n",
    "\n",
    "print(\"ðŸš€ Starting PDF Processing Pipeline...\")\n",
    "pdf_vectorstore = load_and_process_pdf(pdf_file_path)\n",
    "\n",
    "# Create Q&A system\n",
    "pdf_qa_system = create_pdf_qa_system(pdf_vectorstore)\n",
    "\n",
    "# Test the PDF Q&A system\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TESTING PDF Q&A SYSTEM\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Educational questions to test\n",
    "pdf_questions = [\n",
    "    \"What is the address of the premise?\",\n",
    "    \"Can I hang a picture on the wall?\", \n",
    "    \"Can I claim my deposit back if I end the tenancy early?\"\n",
    "]\n",
    "\n",
    "for question in pdf_questions:\n",
    "    print(f\"\\nðŸ”µ INPUT TO PDF Q&A SYSTEM:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"Action: Search PDF content + Generate answer\")\n",
    "    \n",
    "    print(f\"\\nðŸ”µ SEARCHING PDF DATABASE AND CALLING GPT-4O-MINI...\")\n",
    "    \n",
    "    result = pdf_qa_system.invoke({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nðŸ¤– OUTPUT FROM GPT (with PDF context):\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(f\"ðŸ“„ Sources: Page {result['source_documents'][0].metadata.get('page', 'Unknown')} of PDF\")\n",
    "    print(f\"ðŸ“ Source Text Preview: {result['source_documents'][0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be94fc",
   "metadata": {},
   "source": [
    "## 7. Tools - Function Calling\n",
    "**Tools are custom functions the AI can call to perform specific actions like calculations, lookups, or API calls.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6627b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define educational tools\n",
    "@tool\n",
    "def get_student_info(student_id: str) -> str:\n",
    "    \"\"\"Get student information from learning management system\"\"\"\n",
    "    students = {\n",
    "        \"STU001\": {\n",
    "            \"name\": \"Alice Johnson\",\n",
    "            \"email\": \"alice.johnson@university.edu\",\n",
    "            \"major\": \"Computer Science\", \n",
    "            \"gpa\": 3.7,\n",
    "            \"current_courses\": [\"CS101\", \"MATH201\", \"PHYS101\"],\n",
    "            \"support_level\": \"Premium\"\n",
    "        }\n",
    "    }\n",
    "    student = students.get(student_id, {\"error\": \"Student not found\"})\n",
    "    return json.dumps(student, indent=2)\n",
    "\n",
    "@tool\n",
    "def calculate_grade(points_earned: int, total_points: int) -> str:\n",
    "    \"\"\"Calculate percentage grade and letter grade\"\"\"\n",
    "    if total_points == 0:\n",
    "        return \"Error: Total points cannot be zero\"\n",
    "    \n",
    "    percentage = (points_earned / total_points) * 100\n",
    "    \n",
    "    if percentage >= 90:\n",
    "        letter = \"A\"\n",
    "    elif percentage >= 80:\n",
    "        letter = \"B\"\n",
    "    elif percentage >= 70:\n",
    "        letter = \"C\"\n",
    "    elif percentage >= 60:\n",
    "        letter = \"D\"\n",
    "    else:\n",
    "        letter = \"F\"\n",
    "    \n",
    "    return f\"Grade: {percentage:.1f}% ({letter})\"\n",
    "\n",
    "# Test tools directly\n",
    "print(\"ðŸ”§ Testing Tools:\")\n",
    "\n",
    "print(\"\\nðŸ”µ INPUT TO TOOL:\")\n",
    "print(\"Tool: Grade Calculator\")\n",
    "print(\"Input: 85 points out of 100\")\n",
    "\n",
    "grade_result = calculate_grade.invoke({\"points_earned\": 85, \"total_points\": 100})\n",
    "print(f\"ðŸ¤– TOOL OUTPUT: {grade_result}\")\n",
    "\n",
    "print(\"\\nðŸ”µ INPUT TO TOOL:\")\n",
    "print(\"Tool: Student Lookup\")\n",
    "print(\"Input: STU001\")\n",
    "\n",
    "student_result = get_student_info.invoke({\"student_id\": \"STU001\"})\n",
    "print(f\"ðŸ¤– TOOL OUTPUT:\\n{student_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d8a76",
   "metadata": {},
   "source": [
    "## 8. Agents - Autonomous Decision Making\n",
    "**Agents autonomously decide which tools to use and in what order to accomplish complex tasks.**\n",
    "\n",
    "### AgentType Differences\n",
    "\n",
    "| AgentType | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION** | Uses structured JSON-like tool calls (machine-friendly). Chooses tools based only on descriptions, no memory. | User: \"Find student STU001 grade.\" â†’ Agent: outputs structured call `{ \"action\": \"lookup_student\", \"args\": {\"id\": \"STU001\"}}`. |\n",
    "| **ZERO_SHOT_REACT_DESCRIPTION** | Uses free-text reasoning (ReAct) to decide tools. No strict structure. | User: \"What's 25Ã—36?\" â†’ Agent: \"I should use calculator. Action: calculator, Input: 25Ã—36\". |\n",
    "| **CHAT_ZERO_SHOT_REACT_DESCRIPTION** | Same as above but designed for chat models, handles multi-turn back-and-forth. | User: \"What's 2023+7?\" â†’ Agent: \"2030.\" â†’ User (next turn): \"Now divide that by 2.\" â†’ Agent: uses prior answer (2030) â†’ \"1015.\" |\n",
    "| **CONVERSATIONAL_REACT_DESCRIPTION** | Adds conversation memory (remembers past queries + tool use). | User: \"Book me a flight.\" â†’ Agent: calls flight tool. â†’ User (later): \"Make it business class.\" â†’ Agent: recalls last tool call (flight booking) â†’ modifies it with \"business class.\" |\n",
    "| **SELF_ASK_WITH_SEARCH** | Special agent: asks itself clarifying sub-questions, then uses search. | User: \"Who is CEO of company that owns Instagram?\" â†’ Agent: \"Who owns Instagram? Meta.\" â†’ \"Who is CEO of Meta? Zuckerberg.\" |\n",
    "| **OPENAI_FUNCTIONS** | Uses OpenAI's native function-calling API (direct JSON calls). Very reliable. | User: \"Weather in Paris tomorrow.\" â†’ Agent: outputs `{\"name\": \"get_weather\", \"arguments\": {\"city\": \"Paris\", \"date\": \"tomorrow\"}}`. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8eccba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create tools list\n",
    "tools = [get_student_info, calculate_grade]\n",
    "\n",
    "# Initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test agent with complex query\n",
    "query = \"Look up student STU001 and calculate their grade if they got 87 points out of 95 total\"\n",
    "\n",
    "print(\"ðŸ”µ INPUT TO AGENT:\")\n",
    "print(\"Complex Query:\", query)\n",
    "print(\"Available Tools:\", [tool.name for tool in tools])\n",
    "\n",
    "print(\"\\nðŸ”µ AGENT REASONING AND GPT-4O-MINI CALLS...\")\n",
    "print(\"(Agent will make multiple calls to GPT and tools)\")\n",
    "\n",
    "response = agent.run(query)\n",
    "\n",
    "print(\"\\nðŸ¤– FINAL OUTPUT FROM AGENT:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0e328",
   "metadata": {},
   "source": [
    "## 9. Complete Educational Support System\n",
    "**Combines all components (models, prompts, chains, memory, retrieval) into a complete working system.**\n",
    "\n",
    "### EducationalSupportBot Architecture\n",
    "\n",
    "The `EducationalSupportBot` class integrates all LangChain components into a unified system:\n",
    "\n",
    "#### **Initialization Components:**\n",
    "- **LLM**: GPT-4o-mini for all AI processing\n",
    "- **Knowledge Base**: Chroma vector store with educational policies\n",
    "- **Memory**: ConversationBufferMemory for chat history\n",
    "- **QA Chain**: RetrievalQA for policy questions\n",
    "- **Tools**: get_student_info() and calculate_grade() functions\n",
    "- **Agent**: STRUCTURED_CHAT agent with access to tools\n",
    "\n",
    "#### **Smart Query Routing Logic:**\n",
    "The `process_query()` method uses keyword-based intent detection:\n",
    "\n",
    "1. **Policy Keywords** (`\"policy\", \"drop\", \"appeal\", \"probation\", \"requirement\"`)\n",
    "   â†’ **Route**: Knowledge Base Retrieval QA\n",
    "   â†’ **Action**: Search educational policies and generate answer\n",
    "\n",
    "2. **Student/Grade Keywords** (`\"student\", \"lookup\", \"info\", \"grade\", \"calculate\"`)\n",
    "   â†’ **Route**: Agent with Tools\n",
    "   â†’ **Action**: Autonomous tool selection (lookup student or calculate grade)\n",
    "\n",
    "3. **General Support** (all other queries)\n",
    "   â†’ **Route**: Conversation Chain with Memory\n",
    "   â†’ **Action**: General educational guidance with conversation history\n",
    "\n",
    "#### **Testing Strategy:**\n",
    "Tests three distinct query types to demonstrate all routing paths:\n",
    "- Policy question â†’ Knowledge base retrieval\n",
    "- Student/grade query â†’ Agent tool usage  \n",
    "- General support â†’ Conversation memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712b97b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Complete Educational Support Bot...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_student_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Initialize the complete system\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Initializing Complete Educational Support Bot...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m support_bot = \u001b[43mEducationalSupportBot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Test various query types\u001b[39;00m\n\u001b[32m     70\u001b[39m test_queries = [\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the course drop policy?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLook up student STU001 and calculate grade for 92 out of 100 points\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     73\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm feeling overwhelmed with my coursework. Any study tips?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mEducationalSupportBot.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.qa_chain = RetrievalQA.from_chain_type(\n\u001b[32m     17\u001b[39m     llm=\u001b[38;5;28mself\u001b[39m.llm,\n\u001b[32m     18\u001b[39m     chain_type=\u001b[33m\"\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     retriever=\u001b[38;5;28mself\u001b[39m.vectorstore.as_retriever()\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Tools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.tools = [\u001b[43mget_student_info\u001b[49m, calculate_grade]\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Agent\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.agent = initialize_agent(\n\u001b[32m     27\u001b[39m     tools=\u001b[38;5;28mself\u001b[39m.tools,\n\u001b[32m     28\u001b[39m     llm=\u001b[38;5;28mself\u001b[39m.llm,\n\u001b[32m     29\u001b[39m     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n\u001b[32m     30\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'get_student_info' is not defined"
     ]
    }
   ],
   "source": [
    "class EducationalSupportBot:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1, api_key=openai_api_key)\n",
    "        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        \n",
    "        # Knowledge base\n",
    "        self.vectorstore = Chroma.from_texts(\n",
    "            texts=educational_policies,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # Memory for conversations\n",
    "        self.memory = ConversationBufferMemory()\n",
    "        \n",
    "        # QA chain for policies\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever()\n",
    "        )\n",
    "        \n",
    "        # Tools\n",
    "        self.tools = [get_student_info, calculate_grade]\n",
    "        \n",
    "        # Agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=False\n",
    "        )\n",
    "    \n",
    "    def process_query(self, query: str):\n",
    "        \"\"\"Process student query intelligently\"\"\"\n",
    "        \n",
    "        print(f\"ðŸ”µ INPUT TO SUPPORT BOT:\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Simple intent detection\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(word in query_lower for word in [\"policy\", \"drop\", \"appeal\", \"probation\", \"requirement\"]):\n",
    "            # Use retrieval for policy questions\n",
    "            print(\"ðŸŽ¯ Intent: Policy Question â†’ Using Knowledge Base\")\n",
    "            print(\"\\nðŸ”µ SEARCHING KNOWLEDGE BASE...\")\n",
    "            result = self.qa_chain.run(query)\n",
    "            \n",
    "        elif any(word in query_lower for word in [\"student\", \"lookup\", \"info\", \"grade\", \"calculate\"]):\n",
    "            # Use agent for student-related queries\n",
    "            print(\"ðŸŽ¯ Intent: Student/Grade Query â†’ Using Agent with Tools\")\n",
    "            print(\"\\nðŸ”µ CALLING AGENT...\")\n",
    "            result = self.agent.run(query)\n",
    "            \n",
    "        else:\n",
    "            # Use memory conversation for general support\n",
    "            print(\"ðŸŽ¯ Intent: General Support â†’ Using Conversation Memory\")\n",
    "            print(\"\\nðŸ”µ CALLING GPT WITH MEMORY...\")\n",
    "            conversation = ConversationChain(llm=self.llm, memory=self.memory)\n",
    "            result = conversation.invoke({\"input\": query})[\"response\"]\n",
    "        \n",
    "        print(f\"\\nðŸ¤– SUPPORT BOT RESPONSE:\")\n",
    "        print(result)\n",
    "        return result\n",
    "\n",
    "# Initialize the complete system\n",
    "print(\"ðŸš€ Initializing Complete Educational Support Bot...\")\n",
    "support_bot = EducationalSupportBot()\n",
    "\n",
    "# Test various query types\n",
    "test_queries = [\n",
    "    \"What's the course drop policy?\",\n",
    "    \"Look up student STU001 and calculate grade for 92 out of 100 points\", \n",
    "    \"I'm feeling overwhelmed with my coursework. Any study tips?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING COMPLETE SUPPORT BOT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    support_bot.process_query(query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"âœ… LangChain Tutorial Complete!\")\n",
    "print(\"You've learned: Models â†’ Prompts â†’ Chains â†’ Memory â†’ Retrieval â†’ Tools â†’ Agents â†’ Integration\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
